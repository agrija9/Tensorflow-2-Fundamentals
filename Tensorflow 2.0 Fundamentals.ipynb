{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Layer\" class. A Layer encapsulates a state (weights) and some computation (defined in the `call` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True # for autotab in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"y = w*x + b\"\"\"\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        w_init = tf.random_normal_initializer() # random initialization\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"), trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "        \n",
    "    # call method\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b # w*x + b\n",
    "\n",
    "# Instantiate Layer\n",
    "linear_layer = Linear(4,2)\n",
    "\n",
    "# Layer can be treated as a function\n",
    "# Input some data \n",
    "y = linear_layer(tf.ones((2,2)))\n",
    "assert y.shape == (2,4)\n",
    "\n",
    "# Weights are automatically tracked under the 'weights' property\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06665818 -0.09086242 -0.01717797  0.02770316]\n",
      " [ 0.06665818 -0.09086242 -0.01717797  0.02770316]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's good practice to create weights in a separate `build` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"y = w*x + b\"\"\"\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    # add_weight method shortcut for creating weights\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                                 initializer=\"random_normal\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer=\"random_normal\",\n",
    "                                 trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "# Instantiate layer\n",
    "linear_layer = Linear(4) # this means 4 units (weights) for the layer\n",
    "\n",
    "# This also calls \"build(input_shape)\" and creates the weights\n",
    "y = linear_layer(tf.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06665818 -0.09086242 -0.01717797  0.02770316]\n",
      " [ 0.06665818 -0.09086242 -0.01717797  0.02770316]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically retreive the gradients of the weights of a layer by calling it inside a GradientTape. With these gradients one can update the weights of a layer, manually or using an optimizer object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from keras API\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000,784).astype(\"float32\")/255, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.8388214111328125\n",
      "100 6.844223976135254\n",
      "200 6.849213600158691\n",
      "300 6.866031169891357\n",
      "400 6.840302467346191\n",
      "500 6.853946685791016\n",
      "600 6.849768161773682\n",
      "700 6.848651885986328\n",
      "800 6.848796367645264\n",
      "900 6.839184761047363\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear Layer (above) with 10 units\n",
    "linear_layer = Layer(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    \n",
    "    # open a GradientTape (Tf 2 new feature)\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # do forward pass\n",
    "        logits = linear_layer(x) # input data to layer this -> direction\n",
    "        \n",
    "        # compute loss for this batch\n",
    "        loss = loss_fn(y, logits) # target vs computed\n",
    "        \n",
    "        # get gradients of weights with respect of loss\n",
    "        gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "        \n",
    "    # Update weights of linear layer\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "    \n",
    "    # logging\n",
    "    if step % 100 == 0:\n",
    "        print(step, float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights created by layers can be either trainable or non-trainable. They're exposed in the layer properties `trainable_weights` and `non_trainable_weights. This layer has non-trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(Layer):\n",
    "    \"\"\"Returns sum of the inputs\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        \n",
    "        # Create non-trainable weight\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0)) # add inputs\n",
    "        return self.total\n",
    "    \n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2,2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy()) # [2, 2]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a multilayer perceptron\n",
    "# Re-use Linear class\n",
    "class MLP(Layer):\n",
    "    \"\"\"Simple stack of linear layers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # connect layers\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x) # activation\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = MLP()\n",
    "\n",
    "# first call will create the weights\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# weights are recursively tracked\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers can create losses during the forward pass. This is especially useful for regularization losses. The losses created by sublayers are recursively tracked by the parent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=67, shape=(), dtype=float32, numpy=0.29380092>]\n"
     ]
    }
   ],
   "source": [
    "class ActivityRegularization(Layer):\n",
    "    \"\"\"Layer creates an activity sparsity regularization loss\"\"\"\n",
    "    \n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # use add_loss to create a regularization loss\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "    \n",
    "# Use loss layer in MLP block\n",
    "class SparseMLP(Layer):\n",
    "    \"\"\"Stack of Linear Layers with a sparsity \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "print(mlp.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.42427921295166\n",
      "100 2.5380611419677734\n",
      "200 2.4305036067962646\n",
      "300 2.3752405643463135\n",
      "400 2.352739095687866\n",
      "500 2.325277328491211\n",
      "600 2.3527536392211914\n",
      "700 2.3397092819213867\n",
      "800 2.333073377609253\n",
      "900 2.324150323867798\n"
     ]
    }
   ],
   "source": [
    "# The loss here corresponds to the last forward pass\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10,10)))\n",
    "\n",
    "# Use losses in a training loop\n",
    "\n",
    "# Prepare dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000, 784).astype(\"float32\")/255, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Define a MLP\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Train\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # forward pass\n",
    "        logits = mlp(x)\n",
    "        \n",
    "        # external loss value for this batch\n",
    "        loss = loss_fn(y, logits)\n",
    "        \n",
    "        # add losses created during the forward pass\n",
    "        loss += sum(mlp.losses)\n",
    "        \n",
    "        # get gradients of weights wrt the loss\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "        \n",
    "    # update weights of linear layer\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "    \n",
    "    # logging \n",
    "    if step % 100 == 0:\n",
    "        print(step, float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF 2.0 is eager by default. Running eagerly is great for debugging, but you will get better performance by compiling your computation into static graphs. You can compile any function by wrapping it in a tf.function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3349523544311523\n",
      "100 2.3174633979797363\n",
      "200 2.2839698791503906\n",
      "300 2.2750468254089355\n",
      "400 2.271489143371582\n",
      "500 2.2708191871643066\n",
      "600 2.2397618293762207\n",
      "700 2.2710442543029785\n",
      "800 2.2650415897369385\n",
      "900 2.2367138862609863\n"
     ]
    }
   ],
   "source": [
    "# Define a MLP\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Create training step function\n",
    "@tf.function # wrap it to make it fast\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mlp(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "# Prepare dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000, 784).astype(\"float32\")/255, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Run training using step function\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x,y)\n",
    "    if step % 100 == 0:\n",
    "        print(step, float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing VAEs in subclassing style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim)) # gaussian noise\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "class Encoder(Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 latent_dim=32,\n",
    "                 intermediate_dim=64,\n",
    "                 name=\"encoder\",\n",
    "                 **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = Dense(intermediate_dim, activation=\"relu\") # using Dense layers\n",
    "        self.dense_mean = Dense(latent_dim)\n",
    "        self.dense_log_var = Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 name=\"decoder\",\n",
    "                 **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = Dense(intermediate_dim, activation=\"relu\") # using Dense layers\n",
    "        self.dense_output = Dense(original_dim, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 latent_dim=32,\n",
    "                 name=\"autoencoder\",\n",
    "                 **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                           intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5* tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.VariationalAutoEncoder'>\n"
     ]
    }
   ],
   "source": [
    "# Next is to define a training procedure for VAE\n",
    "print(type(vae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python36] *",
   "language": "python",
   "name": "conda-env-Python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
